{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dedbac-848e-45ab-b24f-f4f307fa6abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import threading\n",
    "import time\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e91991f-4d9f-497e-9c92-a79f9bb16091",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Biased MNIST CNN Training - CrossEntropy Version\n",
    "\n",
    "## Model Architecture:\n",
    "- SimpleCNN with 2 convolutional layers followed by a fully connected layer\n",
    "- Input: RGB images (3 channels, 160x160)\n",
    "- Conv1: 3 -> 16 channels, 3x3 kernel, stride 2, padding 1, followed by ReLU and MaxPool\n",
    "- Conv2: 16 -> 32 channels, 3x3 kernel, stride 2, padding 1, followed by ReLU and MaxPool\n",
    "- Final feature map size: 32 x 10 x 10 \n",
    "- Fully connected layer: 3200 -> 10 (one for each digit)\n",
    "\n",
    "## Loss Function:\n",
    "- CrossEntropyLoss\n",
    "- Works directly with class indices (0-9)\n",
    "- Good for multi-class classification problems\n",
    "\n",
    "## Training Details:\n",
    "- Optimizer: Adam with learning rate 0.001\n",
    "- Batch size: 32\n",
    "- Epochs: 20 for subset training (10% of training data)\n",
    "- Dataset: Biased MNIST with correlation level 0.5\n",
    "- Uses data normalization based on dataset statistics\n",
    "\n",
    "## How to Run:\n",
    "1. Place your biased MNIST dataset in the correct paths:\n",
    "   - Default paths assume: 'biased_mnist/full_0.5/trainval'\n",
    "2. Run the subset training function for faster iteration:\n",
    "   - run_biased_mnist_cnn_subset()\n",
    "\n",
    "## Expected Outputs:\n",
    "- Training/validation accuracies and loss will be displayed during training\n",
    "- Model will be saved as 'simple_biased_mnist_cnn_subset_05.pth'\n",
    "\n",
    "## Notes:\n",
    "- Memory monitoring is included to track RAM usage\n",
    "- You can adjust the correlation level by changing paths and filenames\n",
    "- The test on Standard MNIST probably isn't a good idea I am not sure yet. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986452f1-5240-4268-b3bd-7a72770b2794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory monitoring class\n",
    "class MemoryMonitor:\n",
    "    def __init__(self, interval=1.0):\n",
    "        self.interval = interval\n",
    "        self.running = False\n",
    "        self.thread = None\n",
    "        self.max_memory = 0\n",
    "        self.current_memory = 0\n",
    "    \n",
    "    def memory_monitor_func(self):\n",
    "        while self.running:\n",
    "            # Get memory info\n",
    "            process = psutil.Process(os.getpid())\n",
    "            memory_info = process.memory_info()\n",
    "            memory_mb = memory_info.rss / (1024 * 1024)  # converting to mb\n",
    "            \n",
    "            self.current_memory = memory_mb\n",
    "            self.max_memory = max(self.max_memory, memory_mb)\n",
    "            time.sleep(self.interval)\n",
    "    \n",
    "    def start(self):\n",
    "        self.running = True\n",
    "        self.thread = threading.Thread(target=self.memory_monitor_func)\n",
    "        self.thread.daemon = True\n",
    "        self.thread.start()\n",
    "    \n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "        if self.thread:\n",
    "            self.thread.join(timeout=2.0)\n",
    "    \n",
    "    def get_memory_usage(self):\n",
    "        return {\n",
    "            'current': self.current_memory,\n",
    "            'max': self.max_memory\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2156a5-46d2-4dbb-837b-87d181e25809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the BiasedMNISTDataset class with JSON integration\n",
    "class BiasedMNISTDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, json_path=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # load labels from JSON\n",
    "        label_dict = {}\n",
    "        if json_path and os.path.exists(json_path):\n",
    "            try:\n",
    "                with open(json_path, 'r') as f:\n",
    "                    json_data = json.load(f)\n",
    "\n",
    "                if isinstance(json_data, list):\n",
    "                    for item in json_data:\n",
    "                        if isinstance(item, dict) and 'index' in item and 'digit' in item:\n",
    "                            label_dict[item['index']] = item['digit']\n",
    "                print(f\"Loaded {len(label_dict)} labels from JSON file\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading JSON: {e}\")\n",
    "        \n",
    "        # Load the images\n",
    "        if os.path.exists(root_dir):\n",
    "            for filename in os.listdir(root_dir):\n",
    "                if filename.endswith('.jpg'):\n",
    "                    self.image_paths.append(os.path.join(root_dir, filename))\n",
    "\n",
    "                    try:\n",
    "                        index = int(os.path.basename(filename).split('.')[0])\n",
    "                        \n",
    "                        # get label from JSON\n",
    "                        if index in label_dict:\n",
    "                            label = label_dict[index]\n",
    "                        else:\n",
    "                            label = index % 10\n",
    "                        \n",
    "                    except (ValueError, IndexError) as e:\n",
    "                        print(f\"Error parsing filename {filename}: {e}\")\n",
    "                        label = 0\n",
    "                    \n",
    "                    self.labels.append(label)\n",
    "        else:\n",
    "            print(f\"Directory not found: {root_dir}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path)\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0200f033-0179-4697-be1b-b2ef040488e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simplified CNN architecture\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # two convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Activation and pooling\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # After conv1 (stride 2): 160x160 -> 80x80\n",
    "        # After pool: 80x80 -> 40x40\n",
    "        # After conv2 (stride 2): 40x40 -> 20x20\n",
    "        # After pool: 20x20 -> 10x10\n",
    "        # So final feature map size is 32 x 10 x 10\n",
    "        self.fc = nn.Linear(32 * 10 * 10, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2624996-939e-4ff4-836d-c74f06e21cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate dataset statistics with a small sample\n",
    "def calculate_stats_fast(dataset, sample_size=1000):\n",
    "    indices = torch.randperm(len(dataset))[:sample_size]\n",
    "    \n",
    "    # Create a temporary dataloader\n",
    "    mini_loader = DataLoader(\n",
    "        Subset(dataset, indices),\n",
    "        batch_size=100,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    print(f\"Calculating statistics using {sample_size} random samples (fast mode)...\")\n",
    "    \n",
    "    channels_sum = torch.zeros(3)\n",
    "    channels_squared_sum = torch.zeros(3)\n",
    "    num_batches = 0\n",
    "    \n",
    "    # memory monitoring\n",
    "    memory_monitor = MemoryMonitor()\n",
    "    memory_monitor.start()\n",
    "    \n",
    "    try:\n",
    "        # progress bar\n",
    "        progress_bar = tqdm(mini_loader, desc=\"Calculating stats\")\n",
    "        \n",
    "        for data, _ in progress_bar:\n",
    "            # [batch_size, 3, height, width]\n",
    "            channels_sum += torch.mean(data, dim=[0, 2, 3])\n",
    "            channels_squared_sum += torch.mean(data**2, dim=[0, 2, 3])\n",
    "            num_batches += 1\n",
    "            \n",
    "            mem_usage = memory_monitor.get_memory_usage()\n",
    "            progress_bar.set_postfix({'RAM': f\"{mem_usage['current']:.1f}MB\"})\n",
    "    \n",
    "    finally:\n",
    "        memory_monitor.stop()\n",
    "\n",
    "        mem_usage = memory_monitor.get_memory_usage()\n",
    "        print(f\"Statistics calculation - Maximum RAM usage: {mem_usage['max']:.1f}MB\")\n",
    "    \n",
    "    mean = channels_sum / num_batches\n",
    "    std = (channels_squared_sum / num_batches - mean**2)**0.5\n",
    "    \n",
    "    print(f\"Fast statistics calculation complete. Using sample of {sample_size} images.\")\n",
    "    return mean, std\n",
    "\n",
    "def create_subset_loader(dataset, fraction=0.1, batch_size=32):\n",
    "    subset_size = int(len(dataset) * fraction)\n",
    "    indices = torch.randperm(len(dataset))[:subset_size]\n",
    "    \n",
    "    subset_dataset = Subset(dataset, indices)\n",
    "    \n",
    "    # DataLoader\n",
    "    subset_loader = DataLoader(\n",
    "        subset_dataset, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    return subset_loader, subset_size\n",
    "\n",
    "def check_label_range(dataset):\n",
    "    try:\n",
    "        sample_size = min(1000, len(dataset))\n",
    "        indices = torch.randperm(len(dataset))[:sample_size]\n",
    "        \n",
    "        labels = [dataset[i.item()][1] for i in indices]\n",
    "        min_label = min(labels)\n",
    "        max_label = max(labels)\n",
    "        \n",
    "        print(f\"Label range (from sample of {sample_size}): {min_label} to {max_label}\")\n",
    "\n",
    "        if max_label >= 10:\n",
    "            print(f\"WARNING: Found labels outside expected range (0-9): max={max_label}\")\n",
    "            return False\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking label range: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec695e4-0844-4f77-a75c-4a543ee23853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGES MADE: \n",
    "# - Using CrossEntropyLoss instead of MSE\n",
    "# - Removed one-hot encoding as it's not needed for CrossEntropyLoss\n",
    "\n",
    "def train_model(model, train_loader, test_loader, num_epochs=3):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Using CrossEntropyLoss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    memory_monitor = MemoryMonitor()\n",
    "    memory_monitor.start()\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "            \n",
    "            for inputs, labels in progress_bar:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                # Calculate CrossEntropy loss directly with integer labels\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                mem_usage = memory_monitor.get_memory_usage()\n",
    "\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f\"{loss.item():.4f}\",\n",
    "                    'acc': f\"{100 * correct / total:.2f}%\",\n",
    "                    'RAM': f\"{mem_usage['current']:.1f}MB\"\n",
    "                })\n",
    "\n",
    "            epoch_loss = running_loss / len(train_loader)\n",
    "            epoch_acc = 100 * correct / total\n",
    "            mem_usage = memory_monitor.get_memory_usage()\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%, Max RAM: {mem_usage[\"max\"]:.1f}MB')\n",
    "\n",
    "            test_accuracy = evaluate_model(model, test_loader, device, memory_monitor)\n",
    "            print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "    \n",
    "    finally:\n",
    "        memory_monitor.stop()\n",
    "        mem_usage = memory_monitor.get_memory_usage()\n",
    "        print(f\"Maximum RAM usage: {mem_usage['max']:.1f}MB\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec7efa7-ba33-423d-921d-5d5954c19c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device=None, memory_monitor=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    progress_bar = tqdm(test_loader, desc=\"Evaluating\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            postfix = {'acc': f\"{100 * correct / total:.2f}%\"}\n",
    "            if memory_monitor:\n",
    "                mem_usage = memory_monitor.get_memory_usage()\n",
    "                postfix['RAM'] = f\"{mem_usage['current']:.1f}MB\"\n",
    "            \n",
    "            progress_bar.set_postfix(postfix)\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dbfdbc-d1e5-47d4-bbf6-b7d1a579eba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGES MADE:\n",
    "# - Removed \"mse\" from the model filename\n",
    "\n",
    "# Run the complete workflow with full dataset\n",
    "def run_biased_mnist_cnn():\n",
    "    # CHANGE THIS PATH AND RENAME MODEL FILE FOR EACH BIAS LEVEL\n",
    "    base_dir = 'biased_mnist'\n",
    "    train_folder = f\"{base_dir}/full_0.5/trainval\"  # Using full_0.5 for now\n",
    "    test_folder = f\"{base_dir}/full/test\"\n",
    "    train_json_path = f\"{base_dir}/full_0.5/trainval.json\" # Using full_0.5 for now\n",
    "    test_json_path = f\"{base_dir}/full/test.json\"\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    print(\"Creating datasets...\")\n",
    "    train_dataset = BiasedMNISTDataset(train_folder, transform=transform, json_path=train_json_path)\n",
    "    test_dataset = BiasedMNISTDataset(test_folder, transform=transform, json_path=test_json_path)\n",
    "\n",
    "    print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "    \n",
    "    if len(train_dataset) == 0 or len(test_dataset) == 0:\n",
    "        print(\"Error: Empty dataset found. Please check the file paths.\")\n",
    "        return\n",
    "\n",
    "    print(\"Checking label ranges...\")\n",
    "    train_labels_ok = check_label_range(train_dataset)\n",
    "    test_labels_ok = check_label_range(test_dataset)\n",
    "    \n",
    "    if not (train_labels_ok and test_labels_ok):\n",
    "        print(\"WARNING: Label range check failed. Please check the dataset.\")\n",
    "\n",
    "    mean, std = calculate_stats_fast(train_dataset, sample_size=1000)\n",
    "    print(f\"Dataset mean: {mean}\")\n",
    "    print(f\"Dataset std: {std}\")\n",
    "    \n",
    "    normalized_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "    \n",
    "    train_dataset_normalized = BiasedMNISTDataset(train_folder, transform=normalized_transform, json_path=train_json_path)\n",
    "    test_dataset_normalized = BiasedMNISTDataset(test_folder, transform=normalized_transform, json_path=test_json_path)\n",
    "    \n",
    "    # dataloaders\n",
    "    train_loader = DataLoader(train_dataset_normalized, batch_size=32, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset_normalized, batch_size=32, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(f\"Using FULL training dataset with {len(train_dataset_normalized)} images\")\n",
    "    print(f\"Using FULL test dataset with {len(test_dataset_normalized)} images\")\n",
    "    \n",
    "    # the simplified CNN model\n",
    "    model = SimpleCNN(num_classes=10)\n",
    "    print(\"Model architecture:\")\n",
    "    print(model)\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"\\nStarting training...\")\n",
    "    trained_model = train_model(model, train_loader, test_loader, num_epochs=3)\n",
    "    \n",
    "    # Final EVAL\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    final_accuracy = evaluate_model(trained_model, test_loader, device, None)\n",
    "    print(f\"\\nFinal Test Accuracy: {final_accuracy:.2f}%\")\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(trained_model.state_dict(), 'simple_biased_mnist_cnn_full.pth')\n",
    "    print(\"Model saved as 'simple_biased_mnist_cnn_full.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29463e67-2a26-461f-97d7-06c50e5d11f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGES MADE:\n",
    "# - Removed \"mse\" from the model filename\n",
    "\n",
    "# Run the complete workflow with subset of data\n",
    "def run_biased_mnist_cnn_subset():\n",
    "    # SET PATHS CORRECTLY FOR CORRECT BIAS LEVEL\n",
    "    base_dir = 'biased_mnist'\n",
    "    train_folder = f\"{base_dir}/full_0.5/trainval\"  # Using full_0.5 for now\n",
    "    test_folder = f\"{base_dir}/full/test\"\n",
    "    train_json_path = f\"{base_dir}/full_0.5/trainval.json\" # Change when changing correlation levels\n",
    "    test_json_path = f\"{base_dir}/full/test.json\"\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    print(\"Creating datasets...\")\n",
    "    train_dataset = BiasedMNISTDataset(train_folder, transform=transform, json_path=train_json_path)\n",
    "    test_dataset = BiasedMNISTDataset(test_folder, transform=transform, json_path=test_json_path)\n",
    "\n",
    "    print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "    \n",
    "    if len(train_dataset) == 0 or len(test_dataset) == 0:\n",
    "        print(\"Error: Empty dataset found. Please check the file paths.\")\n",
    "        return\n",
    "\n",
    "    print(\"Checking label ranges...\")\n",
    "    train_labels_ok = check_label_range(train_dataset)\n",
    "    test_labels_ok = check_label_range(test_dataset)\n",
    "    \n",
    "    if not (train_labels_ok and test_labels_ok):\n",
    "        print(\"WARNING: Label range check failed. Please check the dataset.\")\n",
    "\n",
    "    mean, std = calculate_stats_fast(train_dataset, sample_size=1000)\n",
    "    print(f\"Dataset mean: {mean}\")\n",
    "    print(f\"Dataset std: {std}\")\n",
    "\n",
    "    normalized_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "    \n",
    "    train_dataset_normalized = BiasedMNISTDataset(train_folder, transform=normalized_transform, json_path=train_json_path)\n",
    "    test_dataset_normalized = BiasedMNISTDataset(test_folder, transform=normalized_transform, json_path=test_json_path)\n",
    "\n",
    "    train_fraction = 0.1  # Use 10% of training data (10,000 IMAGES)\n",
    "    test_fraction = 0.2   # Use 20% of test data (2,000 IMAGES)\n",
    "    \n",
    "    train_loader, train_subset_size = create_subset_loader(\n",
    "        train_dataset_normalized, \n",
    "        fraction=train_fraction, \n",
    "        batch_size=32\n",
    "    )\n",
    "    \n",
    "    test_loader, test_subset_size = create_subset_loader(\n",
    "        test_dataset_normalized, \n",
    "        fraction=test_fraction, \n",
    "        batch_size=32\n",
    "    )\n",
    "    \n",
    "    print(f\"Using {train_subset_size} training images ({train_fraction*100:.1f}% of dataset)\")\n",
    "    print(f\"Using {test_subset_size} test images ({test_fraction*100:.1f}% of dataset)\")\n",
    "    \n",
    "    # the simplified CNN model\n",
    "    model = SimpleCNN(num_classes=10)\n",
    "    print(\"Model architecture:\")\n",
    "    print(model)\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"\\nStarting training...\")\n",
    "    trained_model = train_model(model, train_loader, test_loader, num_epochs=20)\n",
    "    \n",
    "    # Final EVAL\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    final_accuracy = evaluate_model(trained_model, test_loader, device, None)\n",
    "    print(f\"\\nFinal Test Accuracy: {final_accuracy:.2f}%\")\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(trained_model.state_dict(), 'simple_biased_mnist_cnn_subset_05.pth')\n",
    "    print(\"Model saved as 'simple_biased_mnist_cnn_subset_05.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc2e8b2-c4c8-4645-80fb-2564b34e5b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(model_path):\n",
    "    model = SimpleCNN(num_classes=10)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_standard_mnist():\n",
    "    # Standard normalization for MNIST\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((160, 160)),  # Resize to match our biased MNIST images\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    mnist_test = torchvision.datasets.MNIST(\n",
    "        root='./data', \n",
    "        train=False, \n",
    "        download=True, \n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    class MNISTtoRGB(torch.utils.data.Dataset):\n",
    "        def __init__(self, mnist_dataset):\n",
    "            self.mnist_dataset = mnist_dataset\n",
    "            \n",
    "        def __len__(self):\n",
    "            return len(self.mnist_dataset)\n",
    "            \n",
    "        def __getitem__(self, idx):\n",
    "            img, label = self.mnist_dataset[idx]\n",
    "            rgb_img = torch.cat([img, img, img], dim=0)\n",
    "            return rgb_img, label\n",
    "    \n",
    "    rgb_mnist_test = MNISTtoRGB(mnist_test)\n",
    "    return rgb_mnist_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e40411-1717-4ef4-802b-cfdba20009c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGES MADE:\n",
    "# - Removed \"mse\" from model filenames\n",
    "\n",
    "def test_on_standard_mnist():\n",
    "    model_01_path = 'simple_biased_mnist_cnn_subset_01.pth'\n",
    "    model_05_path = 'simple_biased_mnist_cnn_subset_05.pth'\n",
    "    try:\n",
    "        model_01 = load_trained_model(model_01_path)\n",
    "        print(f\"Successfully loaded model trained on correlation level 0.1\")\n",
    "    except:\n",
    "        print(f\"Could not load model from {model_01_path}\")\n",
    "        model_01 = None\n",
    "        \n",
    "    try:\n",
    "        model_05 = load_trained_model(model_05_path)\n",
    "        print(f\"Successfully loaded model trained on correlation level 0.5\")\n",
    "    except:\n",
    "        print(f\"Could not load model from {model_05_path}\")\n",
    "        model_05 = None\n",
    "\n",
    "    print(\"Loading standard MNIST test set...\")\n",
    "    mnist_dataset = load_standard_mnist()\n",
    "    mnist_loader = DataLoader(mnist_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "    print(f\"Loaded {len(mnist_dataset)} standard MNIST test images\")\n",
    "\n",
    "    print(\"\\nEvaluating models on standard (unbiased) MNIST:\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    if model_01 is not None:\n",
    "        print(\"\\nEvaluating model trained on correlation level 0.1:\")\n",
    "        accuracy_01 = evaluate_model(model_01, mnist_loader, None, None)\n",
    "        results[\"0.1\"] = accuracy_01\n",
    "        print(f\"Accuracy on standard MNIST: {accuracy_01:.2f}%\")\n",
    "        \n",
    "    if model_05 is not None:\n",
    "        print(\"\\nEvaluating model trained on correlation level 0.5:\")\n",
    "        accuracy_05 = evaluate_model(model_05, mnist_loader, None, None)\n",
    "        results[\"0.5\"] = accuracy_05\n",
    "        print(f\"Accuracy on standard MNIST: {accuracy_05:.2f}%\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nSummary of model performance on standard MNIST:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"| Correlation Level | Biased Test Acc | Standard MNIST Acc |\")\n",
    "    print(\"|-------------------|-----------------|-------------------|\")\n",
    "    \n",
    "    if \"0.1\" in results:\n",
    "        print(f\"| 0.1               | 37.55%          | {results['0.1']:.2f}%             |\")\n",
    "    \n",
    "    if \"0.5\" in results:\n",
    "        print(f\"| 0.5               | 18.00%          | {results['0.5']:.2f}%             |\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(\"Note: A model that relies heavily on bias features will perform poorly on standard MNIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590912e3-66a1-4721-9ec5-5320095411f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function for full dataset training\n",
    "# run_biased_mnist_cnn()\n",
    "\n",
    "# Execute the function for subset training\n",
    "run_biased_mnist_cnn_subset()\n",
    "\n",
    "# Run the test on standard mnist as counterfactual set\n",
    "# test_on_standard_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7017d109-22cb-4730-9f34-9957c025915a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3499aa-ff59-4684-b5a7-e024821a4020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1560ef-c614-42f2-af54-f10a1248ed90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7f9582-a82d-46fb-a892-2e6c5746bac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548db870-b3ec-47c9-848d-97daa4710342",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
