{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59467043-4177-4b7c-aaa6-ed620f282351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32b474f3-1f3f-442a-ade1-f0c4a157e51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created output directories in Data\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import libraries and setup\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create directory structure for outputs\n",
    "data_dir = \"Data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "pca_dir = os.path.join(data_dir, \"PCA\")\n",
    "pacmap_dir = os.path.join(data_dir, \"PACMAP\")\n",
    "umap_dir = os.path.join(data_dir, \"UMAP\")\n",
    "\n",
    "for directory in [pca_dir, pacmap_dir, umap_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "print(f\"Created output directories in {data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6e69f6b-76a7-4ea1-ac17-842c2ddf52b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Define data loading function\n",
    "def load_biased_mnist(base_path, bias_level='full', split='train', \n",
    "                      max_samples=None, resize=None, normalize=True):\n",
    "    if split == 'test':\n",
    "        image_folder = os.path.join(base_path, 'test')\n",
    "        json_path = None\n",
    "    else:\n",
    "        image_folder = os.path.join(base_path, bias_level, 'trainval')\n",
    "        json_path = os.path.join(base_path, bias_level, 'trainval.json')\n",
    "    \n",
    "    if split in ['train', 'val']:\n",
    "        indices_path = os.path.join(base_path, f\"{split}_ixs.json\")\n",
    "        try:\n",
    "            with open(indices_path, 'r') as f:\n",
    "                indices = json.load(f)\n",
    "                if max_samples is not None:\n",
    "                    indices = indices[:max_samples]\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Could not find indices file {indices_path}\")\n",
    "            return np.array([]), np.array([]), []\n",
    "    \n",
    "    metadata = []\n",
    "    if json_path and os.path.exists(json_path):\n",
    "        try:\n",
    "            with open(json_path, 'r') as f:\n",
    "                metadata_list = json.load(f)\n",
    "                metadata_dict = {item['index']: item for item in metadata_list}\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Could not find metadata file {json_path}\")\n",
    "            metadata_dict = {}\n",
    "    else:\n",
    "        metadata_dict = {}\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "    image_metadata = []\n",
    "    \n",
    "    if split in ['train', 'val']:\n",
    "        print(f\"Loading {len(indices)} {split} images from {image_folder}...\")\n",
    "        for idx in tqdm(indices, desc=f\"Loading {split} data\"):\n",
    "            img_path = os.path.join(image_folder, f\"{idx}.jpg\")\n",
    "            if not os.path.exists(img_path):\n",
    "                print(f\"Warning: Image {img_path} not found.\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                if resize:\n",
    "                    img = img.resize(resize)\n",
    "                img_array = np.array(img)\n",
    "                \n",
    "                if json_path and idx in metadata_dict:\n",
    "                    item = metadata_dict[idx]\n",
    "                    label = item['digit']\n",
    "                    image_metadata.append(item)\n",
    "                else:\n",
    "                    label = int(os.path.basename(img_path).split('.')[0]) % 10\n",
    "                    image_metadata.append(None)\n",
    "                \n",
    "                images.append(img_array)\n",
    "                labels.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_path}: {e}\")\n",
    "    else:\n",
    "        image_files = [f for f in os.listdir(image_folder) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        image_files.sort()\n",
    "        if max_samples is not None:\n",
    "            image_files = image_files[:max_samples]\n",
    "        \n",
    "        print(f\"Loading {len(image_files)} test images from {image_folder}...\")\n",
    "        for filename in tqdm(image_files, desc=\"Loading test data\"):\n",
    "            img_path = os.path.join(image_folder, filename)\n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                if resize:\n",
    "                    img = img.resize(resize)\n",
    "                img_array = np.array(img)\n",
    "                \n",
    "                label = int(filename.split('.')[0]) % 10\n",
    "                \n",
    "                images.append(img_array)\n",
    "                labels.append(label)\n",
    "                image_metadata.append(None)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_path}: {e}\")\n",
    "    \n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels, dtype=np.int64)\n",
    "    \n",
    "    if normalize and images.size > 0:\n",
    "        images = images.astype(np.float32) / 255.0\n",
    "    \n",
    "    print(f\"Successfully loaded {len(images)} images with shape {images.shape}\")\n",
    "    \n",
    "    if len(labels) > 0:\n",
    "        label_counts = np.bincount(labels, minlength=10)\n",
    "        print(\"Label distribution:\")\n",
    "        for i, count in enumerate(label_counts):\n",
    "            print(f\"  Digit {i}: {count} images ({count/len(labels)*100:.1f}%)\")\n",
    "    \n",
    "    return images, labels, image_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "850e31a5-e1b2-45d7-ae88-21887b9fad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Define summary functions\n",
    "def create_dataset_summary(datasets, bias_levels):\n",
    "    summary_data = []\n",
    "    \n",
    "    for bias_level in bias_levels:\n",
    "        if bias_level in datasets:\n",
    "            data = datasets[bias_level]\n",
    "            \n",
    "            summary_data.append({\n",
    "                'bias_level': bias_level,\n",
    "                'num_samples': len(data['images']),\n",
    "                'image_shape': str(data['images'].shape[1:]),\n",
    "                'num_classes': len(np.unique(data['labels'])),\n",
    "                'data_type': str(data['images'].dtype)\n",
    "            })\n",
    "            \n",
    "            label_counts = np.bincount(data['labels'], minlength=10)\n",
    "            for i, count in enumerate(label_counts):\n",
    "                summary_data.append({\n",
    "                    'bias_level': bias_level,\n",
    "                    'digit': i,\n",
    "                    'count': count,\n",
    "                    'percentage': count/len(data['labels'])*100 if len(data['labels']) > 0 else 0\n",
    "                })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df.to_csv(os.path.join(data_dir, \"dataset_summary.csv\"), index=False)\n",
    "    print(f\"Dataset summary saved to {os.path.join(data_dir, 'dataset_summary.csv')}\")\n",
    "\n",
    "def create_combined_results_files():\n",
    "    # Combine PCA data\n",
    "    pca_files = [f for f in os.listdir(pca_dir) if f.startswith('pca_data_')]\n",
    "    if pca_files:\n",
    "        pca_dfs = []\n",
    "        for file in pca_files:\n",
    "            bias_level = file.replace('pca_data_', '').replace('.csv', '')\n",
    "            df = pd.read_csv(os.path.join(pca_dir, file))\n",
    "            df['bias_level'] = bias_level\n",
    "            pca_dfs.append(df)\n",
    "        \n",
    "        combined_pca = pd.concat(pca_dfs, ignore_index=True)\n",
    "        combined_pca.to_csv(os.path.join(data_dir, \"combined_pca.csv\"), index=False)\n",
    "        print(f\"Combined PCA data saved to {os.path.join(data_dir, 'combined_pca.csv')}\")\n",
    "    \n",
    "    # Combine PACMAP data\n",
    "    pacmap_files = [f for f in os.listdir(pacmap_dir) if f.startswith('pacmap_')]\n",
    "    if pacmap_files:\n",
    "        pacmap_dfs = []\n",
    "        for file in pacmap_files:\n",
    "            bias_level = file.replace('pacmap_', '').replace('.csv', '')\n",
    "            df = pd.read_csv(os.path.join(pacmap_dir, file))\n",
    "            df['bias_level'] = bias_level\n",
    "            pacmap_dfs.append(df)\n",
    "        \n",
    "        combined_pacmap = pd.concat(pacmap_dfs, ignore_index=True)\n",
    "        combined_pacmap.to_csv(os.path.join(data_dir, \"combined_pacmap.csv\"), index=False)\n",
    "        print(f\"Combined PACMAP data saved to {os.path.join(data_dir, 'combined_pacmap.csv')}\")\n",
    "    \n",
    "    # Combine UMAP data\n",
    "    umap_files = [f for f in os.listdir(umap_dir) if f.startswith('umap_')]\n",
    "    if umap_files:\n",
    "        umap_dfs = []\n",
    "        for file in umap_files:\n",
    "            bias_level = file.replace('umap_', '').replace('.csv', '')\n",
    "            df = pd.read_csv(os.path.join(umap_dir, file))\n",
    "            df['bias_level'] = bias_level\n",
    "            umap_dfs.append(df)\n",
    "        \n",
    "        combined_umap = pd.concat(umap_dfs, ignore_index=True)\n",
    "        combined_umap.to_csv(os.path.join(data_dir, \"combined_umap.csv\"), index=False)\n",
    "        print(f\"Combined UMAP data saved to {os.path.join(data_dir, 'combined_umap.csv')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d9f1273-1783-483e-8617-f5ef8f6e16a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: PCA Implementation\n",
    "def perform_pca_and_save(images, labels, bias_level, n_components=None):\n",
    "    print(f\"Performing PCA for {bias_level}...\")\n",
    "    \n",
    "    n_samples = images.shape[0]\n",
    "    images_flattened = images.reshape(n_samples, -1)\n",
    "    print(f\"Flattened image shape: {images_flattened.shape}\")\n",
    "    \n",
    "    max_components = min(n_samples, images_flattened.shape[1])\n",
    "    \n",
    "    if n_components is None or n_components > max_components:\n",
    "        n_components = max_components\n",
    "        print(f\"Setting n_components to maximum possible: {n_components}\")\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    images_scaled = scaler.fit_transform(images_flattened)\n",
    "    \n",
    "    pca = PCA(n_components=n_components)\n",
    "    images_pca = pca.fit_transform(images_scaled)\n",
    "    print(f\"PCA-transformed data shape: {images_pca.shape}\")\n",
    "    \n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "    print(f\"Total variance explained by {n_components} components: {cumulative_variance[-1]:.4f}\")\n",
    "    \n",
    "    variance_thresholds = {}\n",
    "    for threshold in [0.8, 0.9, 0.95]:\n",
    "        n_comp = np.argmax(cumulative_variance >= threshold) + 1 if any(cumulative_variance >= threshold) else n_components\n",
    "        variance_thresholds[threshold] = n_comp\n",
    "        print(f\"Components needed for {threshold*100:.0f}% variance: {n_comp}\")\n",
    "    \n",
    "    # Save results\n",
    "    output_components = min(50, images_pca.shape[1])\n",
    "    pca_data = pd.DataFrame(images_pca[:, :output_components])\n",
    "    pca_data.columns = [f'PC{i+1}' for i in range(output_components)]\n",
    "    pca_data['label'] = labels\n",
    "    pca_data.to_csv(os.path.join(pca_dir, f\"pca_data_{bias_level}.csv\"), index=False)\n",
    "    \n",
    "    variance_df = pd.DataFrame({\n",
    "        'component': range(1, len(explained_variance_ratio) + 1),\n",
    "        'explained_variance_ratio': explained_variance_ratio,\n",
    "        'cumulative_variance': cumulative_variance\n",
    "    })\n",
    "    variance_df.to_csv(os.path.join(pca_dir, f\"pca_variance_{bias_level}.csv\"), index=False)\n",
    "    \n",
    "    variance_threshold_df = pd.DataFrame([\n",
    "        {'bias_level': bias_level, 'threshold': t, 'components_needed': n}\n",
    "        for t, n in variance_thresholds.items()\n",
    "    ])\n",
    "    variance_threshold_df.to_csv(os.path.join(pca_dir, f\"pca_thresholds_{bias_level}.csv\"), index=False)\n",
    "    \n",
    "    loadings_components = min(10, pca.components_.shape[0])\n",
    "    loadings_df = pd.DataFrame(pca.components_[:loadings_components, :])\n",
    "    loadings_df.index = [f'PC{i+1}' for i in range(loadings_components)]\n",
    "    loadings_df.to_csv(os.path.join(pca_dir, f\"pca_loadings_{bias_level}.csv\"))\n",
    "    \n",
    "    print(f\"PCA results for {bias_level} saved to {pca_dir}\")\n",
    "    \n",
    "    return pca, images_pca, explained_variance_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1eb9580-8626-4df4-b717-6505b26f54f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: PACMAP Implementation\n",
    "def perform_pacmap_and_save(images, labels, bias_level):\n",
    "    try:\n",
    "        import pacmap\n",
    "        print(f\"Performing PACMAP for {bias_level}...\")\n",
    "        \n",
    "        n_samples = images.shape[0]\n",
    "        images_flattened = images.reshape(n_samples, -1)\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        images_scaled = scaler.fit_transform(images_flattened)\n",
    "        \n",
    "        embedder = pacmap.PaCMAP(n_components=3, n_neighbors=10, MN_ratio=0.5, FP_ratio=2.0)\n",
    "        embedding = embedder.fit_transform(images_scaled)\n",
    "        \n",
    "        # Save results\n",
    "        embedding_df = pd.DataFrame(embedding, columns=['PACMAP1', 'PACMAP2', 'PACMAP3'])\n",
    "        embedding_df['label'] = labels\n",
    "        \n",
    "        embedding_df.to_csv(os.path.join(pacmap_dir, f\"pacmap_{bias_level}.csv\"), index=False)\n",
    "        print(f\"PACMAP results for {bias_level} saved to {pacmap_dir}\")\n",
    "        \n",
    "        return embedding\n",
    "    \n",
    "    except ImportError:\n",
    "        print(\"PACMAP not installed. To install, run: pip install pacmap\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a024e792-49af-4680-9710-c98c26c2c8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: UMAP Implementation\n",
    "def perform_umap_and_save(images, labels, bias_level):\n",
    "    try:\n",
    "        import umap\n",
    "        print(f\"Performing UMAP for {bias_level}...\")\n",
    "        \n",
    "        n_samples = images.shape[0]\n",
    "        images_flattened = images.reshape(n_samples, -1)\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        images_scaled = scaler.fit_transform(images_flattened)\n",
    "        \n",
    "        reducer = umap.UMAP(n_components=3, n_neighbors=15, min_dist=0.1, random_state=42)\n",
    "        embedding = reducer.fit_transform(images_scaled)\n",
    "        \n",
    "        # Save results\n",
    "        embedding_df = pd.DataFrame(embedding, columns=['UMAP1', 'UMAP2', 'UMAP3'])\n",
    "        embedding_df['label'] = labels\n",
    "        \n",
    "        embedding_df.to_csv(os.path.join(umap_dir, f\"umap_{bias_level}.csv\"), index=False)\n",
    "        print(f\"UMAP results for {bias_level} saved to {umap_dir}\")\n",
    "        \n",
    "        return embedding\n",
    "    \n",
    "    except ImportError:\n",
    "        print(\"UMAP not installed. To install, run: pip install umap-learn\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2aacd2f1-2993-4a06-be07-eca3c3ebb3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Main execution function\n",
    "def main():\n",
    "    bias_levels = ['full_0.1', 'full_0.5', 'full_0.99']\n",
    "    \n",
    "    datasets = {}\n",
    "    \n",
    "    for bias_level in bias_levels:\n",
    "        print(f\"\\nProcessing bias level: {bias_level}\")\n",
    "        \n",
    "        # Load data\n",
    "        images, labels, metadata = load_biased_mnist(\n",
    "            \"biased_mnist\",\n",
    "            bias_level=bias_level,\n",
    "            split='train',\n",
    "            max_samples=1000,\n",
    "            resize=(28, 28)\n",
    "        )\n",
    "        \n",
    "        if len(images) == 0:\n",
    "            print(f\"No images found for bias level {bias_level}, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        datasets[bias_level] = {\n",
    "            'images': images,\n",
    "            'labels': labels,\n",
    "            'metadata': metadata\n",
    "        }\n",
    "        \n",
    "        # Perform dimensionality reduction techniques\n",
    "        perform_pca_and_save(images, labels, bias_level)\n",
    "        perform_pacmap_and_save(images, labels, bias_level)\n",
    "        perform_umap_and_save(images, labels, bias_level)\n",
    "    \n",
    "    # Create summary files\n",
    "    create_dataset_summary(datasets, bias_levels)\n",
    "    create_combined_results_files()\n",
    "    \n",
    "    print(\"\\nAll processing complete. Data saved to the 'Data' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8ed7645-c55f-4476-9b4d-dd4ad085fc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing bias level: full_0.1\n",
      "Loading 1000 train images from biased_mnist/full_0.1/trainval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading train data: 100%|█████████████████| 1000/1000 [00:00<00:00, 1244.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 1000 images with shape (1000, 28, 28, 3)\n",
      "Label distribution:\n",
      "  Digit 0: 92 images (9.2%)\n",
      "  Digit 1: 109 images (10.9%)\n",
      "  Digit 2: 98 images (9.8%)\n",
      "  Digit 3: 96 images (9.6%)\n",
      "  Digit 4: 118 images (11.8%)\n",
      "  Digit 5: 78 images (7.8%)\n",
      "  Digit 6: 108 images (10.8%)\n",
      "  Digit 7: 95 images (9.5%)\n",
      "  Digit 8: 90 images (9.0%)\n",
      "  Digit 9: 116 images (11.6%)\n",
      "Performing PCA for full_0.1...\n",
      "Flattened image shape: (1000, 2352)\n",
      "Setting n_components to maximum possible: 1000\n",
      "PCA-transformed data shape: (1000, 1000)\n",
      "Total variance explained by 1000 components: 1.0000\n",
      "Components needed for 80% variance: 157\n",
      "Components needed for 90% variance: 293\n",
      "Components needed for 95% variance: 438\n",
      "PCA results for full_0.1 saved to Data/PCA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n",
      "Note: `n_components != 2` have not been thoroughly tested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing PACMAP for full_0.1...\n",
      "PACMAP results for full_0.1 saved to Data/PACMAP\n",
      "Performing UMAP for full_0.1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justincoffey/anaconda3/envs/tf-macos/lib/python3.10/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/justincoffey/anaconda3/envs/tf-macos/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP results for full_0.1 saved to Data/UMAP\n",
      "\n",
      "Processing bias level: full_0.5\n",
      "Loading 1000 train images from biased_mnist/full_0.5/trainval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading train data: 100%|█████████████████| 1000/1000 [00:00<00:00, 1232.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 1000 images with shape (1000, 28, 28, 3)\n",
      "Label distribution:\n",
      "  Digit 0: 92 images (9.2%)\n",
      "  Digit 1: 109 images (10.9%)\n",
      "  Digit 2: 98 images (9.8%)\n",
      "  Digit 3: 96 images (9.6%)\n",
      "  Digit 4: 118 images (11.8%)\n",
      "  Digit 5: 78 images (7.8%)\n",
      "  Digit 6: 108 images (10.8%)\n",
      "  Digit 7: 95 images (9.5%)\n",
      "  Digit 8: 90 images (9.0%)\n",
      "  Digit 9: 116 images (11.6%)\n",
      "Performing PCA for full_0.5...\n",
      "Flattened image shape: (1000, 2352)\n",
      "Setting n_components to maximum possible: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: `n_components != 2` have not been thoroughly tested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA-transformed data shape: (1000, 1000)\n",
      "Total variance explained by 1000 components: 1.0000\n",
      "Components needed for 80% variance: 145\n",
      "Components needed for 90% variance: 275\n",
      "Components needed for 95% variance: 418\n",
      "PCA results for full_0.5 saved to Data/PCA\n",
      "Performing PACMAP for full_0.5...\n",
      "PACMAP results for full_0.5 saved to Data/PACMAP\n",
      "Performing UMAP for full_0.5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justincoffey/anaconda3/envs/tf-macos/lib/python3.10/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/justincoffey/anaconda3/envs/tf-macos/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP results for full_0.5 saved to Data/UMAP\n",
      "\n",
      "Processing bias level: full_0.99\n",
      "Loading 1000 train images from biased_mnist/full_0.99/trainval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading train data: 100%|█████████████████| 1000/1000 [00:00<00:00, 1333.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 1000 images with shape (1000, 28, 28, 3)\n",
      "Label distribution:\n",
      "  Digit 0: 92 images (9.2%)\n",
      "  Digit 1: 109 images (10.9%)\n",
      "  Digit 2: 98 images (9.8%)\n",
      "  Digit 3: 96 images (9.6%)\n",
      "  Digit 4: 118 images (11.8%)\n",
      "  Digit 5: 78 images (7.8%)\n",
      "  Digit 6: 108 images (10.8%)\n",
      "  Digit 7: 95 images (9.5%)\n",
      "  Digit 8: 90 images (9.0%)\n",
      "  Digit 9: 116 images (11.6%)\n",
      "Performing PCA for full_0.99...\n",
      "Flattened image shape: (1000, 2352)\n",
      "Setting n_components to maximum possible: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: `n_components != 2` have not been thoroughly tested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA-transformed data shape: (1000, 1000)\n",
      "Total variance explained by 1000 components: 1.0000\n",
      "Components needed for 80% variance: 91\n",
      "Components needed for 90% variance: 196\n",
      "Components needed for 95% variance: 324\n",
      "PCA results for full_0.99 saved to Data/PCA\n",
      "Performing PACMAP for full_0.99...\n",
      "PACMAP results for full_0.99 saved to Data/PACMAP\n",
      "Performing UMAP for full_0.99...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justincoffey/anaconda3/envs/tf-macos/lib/python3.10/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/justincoffey/anaconda3/envs/tf-macos/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP results for full_0.99 saved to Data/UMAP\n",
      "Dataset summary saved to Data/dataset_summary.csv\n",
      "Combined PCA data saved to Data/combined_pca.csv\n",
      "Combined PACMAP data saved to Data/combined_pacmap.csv\n",
      "Combined UMAP data saved to Data/combined_umap.csv\n",
      "\n",
      "All processing complete. Data saved to the 'Data' directory.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Execution\n",
    "# Run the main function if executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "else:\n",
    "    print(\"Run main() to process the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b174a1a-fce5-42d1-8616-01933d66e015",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
